===== STANDARD ATTENTION BENCHMARKS (PART 1) =====
Testing modern LLM configurations...
Testing sliding window attention...
Profiling: standard/window-128
==PROF== Connected to process 12454 (/home/accel-sim-framework/util/tracer_nvbit/applications/benchmark_flash_attention/build/flash_attention_benchmark_standard)
# Devices

## [0] `NVIDIA A30`
* SM Version: 800 (PTX Version: 800)
* Number of SMs: 56
* SM Default Clock Rate: 1440 MHz
* Global Memory: 23657 MiB Free / 24060 MiB Total
* Global Memory Bus Peak: 933 GB/sec (3072-bit DDR @1215MHz)
* Max Shared Memory: 164 KiB/SM, 48 KiB/Block
* L2 Cache Size: 24576 KiB
* Maximum Active Blocks: 32/SM
* Maximum Active Threads: 2048/SM, 1024/Block
* Available Registers: 65536/SM, 65536/Block
* ECC Enabled: Yes

# Log

```
Run:  [1/10] run_mha_fwd [Device=0 num_seqs=1 seq_len=512 num_heads=32 num_kv_heads=8 head_size=128 window_left=128 window_right=128 causal=0]
==PROF== Profiling "distribution_elementwise_grid..." - 0: 0%....50%....100% - 13 passes
==PROF== Profiling "distribution_elementwise_grid..." - 1: 0%....50%....100% - 13 passes
==PROF== Profiling "distribution_elementwise_grid..." - 2: 