===== KV CACHE ATTENTION BENCHMARKS =====
Testing modern LLM configurations with KV cache...
Testing sliding window with KV cache...
Profiling: kvcache/window-128
==PROF== Connected to process 12487 (/home/accel-sim-framework/util/tracer_nvbit/applications/benchmark_flash_attention/build/flash_attention_benchmark_kvcache)
# Devices

## [0] `NVIDIA A30`
* SM Version: 800 (PTX Version: 800)
* Number of SMs: 56
* SM Default Clock Rate: 1440 MHz
* Global Memory: 23651 MiB Free / 24060 MiB Total
* Global Memory Bus Peak: 933 GB/sec (3072-bit DDR @1215MHz)
* Max Shared Memory: 164 KiB/SM, 48 KiB/Block
* L2 Cache Size: 24576 KiB
* Maximum Active Blocks: 32/SM
* Maximum Active Threads: 2048/SM, 1024/Block
* Available Registers: 65536/SM, 65536/Block
* ECC Enabled: Yes

# Log

```
Run:  [1/2] run_mha_kvcache [Device=0 num_seqs=1 seq_len=16384 num_heads=32 num_kv_heads=8 head_size=128 page_size=256 window_left=128 window_right=128 causal=0]
==PROF== Profiling "vectorized_elementwise_kernel" - 0: 0%....50%....100% - 13 passes
==PROF== Profiling "elementwise_kernel_with_index" - 1: 0%....50%....100% - 13 passes
==PROF== Profiling "flash_fwd_splitkv_kernel" - 2: 